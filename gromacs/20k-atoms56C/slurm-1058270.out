[node011:22104] mca: base: component_find: unable to open /cm/shared/apps/openmpi/gcc/64/1.10.3/lib64/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[node010:17352] mca: base: component_find: unable to open /cm/shared/apps/openmpi/gcc/64/1.10.3/lib64/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[node011:22105] mca: base: component_find: unable to open /cm/shared/apps/openmpi/gcc/64/1.10.3/lib64/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[node010:17353] mca: base: component_find: unable to open /cm/shared/apps/openmpi/gcc/64/1.10.3/lib64/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[node011:22106] mca: base: component_find: unable to open /cm/shared/apps/openmpi/gcc/64/1.10.3/lib64/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[node010:17354] mca: base: component_find: unable to open /cm/shared/apps/openmpi/gcc/64/1.10.3/lib64/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[node011:22107] mca: base: component_find: unable to open /cm/shared/apps/openmpi/gcc/64/1.10.3/lib64/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[node010:17355] mca: base: component_find: unable to open /cm/shared/apps/openmpi/gcc/64/1.10.3/lib64/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
                      :-) GROMACS - mdrun_mpi, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      mdrun_mpi, version 2016.3
Executable:   /cm/shared/apps/gromacs/2016.3/bin/mdrun_mpi
Data prefix:  /cm/shared/apps/gromacs/2016.3
Working dir:  /home/tntech.edu/sw-arkerley/gromacs/20k-atoms56C
Command line:
  mdrun_mpi 7 -s benchmark.tpr -g bench_mpi.log


Back Off! I just backed up bench_mpi.log to ./#bench_mpi.log.1#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 2 nodes with total 56 cores, 56 logical cores, 0 compatible GPUs
  Cores per node:           28
  Logical cores per node:   28
  Compatible GPUs per node:  0
Hardware detected on host node010 (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Basic

Reading file benchmark.tpr, VERSION 5.1.4 (single precision)
Note: file tpx version 103, software tpx version 110
Changing nstlist from 10 to 25, rlist from 1.2 to 1.24

Using 8 MPI processes
Using 7 OpenMP threads per MPI process


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Great Red Owns Many ACres of Sand'
50000 steps,    100.0 ps.

step 75 Turning on dynamic load balancing, because the performance loss due to load imbalance is 2.9 %.


Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

 Average load imbalance: 2.0 %
 Part of the total run time spent waiting due to load imbalance: 0.9 %
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: Y 0 %


               Core t (s)   Wall t (s)        (%)
       Time:     3990.072       71.251     5600.0
                 (ns/day)    (hour/ns)
Performance:      121.263        0.198

GROMACS reminds you: "Don't pay any attention to what they write about you. Just measure it in inches." (Andy Warhol)

